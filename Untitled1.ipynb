{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "699552cf-2d2a-4812-9831-8e0b9441bfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from general_extraction_methods import *\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c20b6e52-8f3c-4007-9224-2afda36afc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_row_data(raw_data_df, categorized_df, y_margin = 100):\n",
    "    print('testing new extraction method!!!')\n",
    "    \n",
    "    rows_df = raw_data_df[raw_data_df['group_index'].apply(lambda x: x in set(categorized_df['group_index']))]\n",
    "    rows_df = rows_df[['text', 'combined_block_text', 'group_index', 'coord_item', 'final_coord_block_par_line_group']]\n",
    "    \n",
    "    rows_df['text'] = rows_df['text'].apply(lambda x: '$0' if x == 'SO' else x) # hardcoded rule, might have to add/remove\n",
    "    \n",
    "    rows_w_numbers = set(rows_df[rows_df['text'].apply(lambda x: any(char.isdigit() for char in x) | (x == '-'))]['group_index']) # limits to rows with at least 1 number\n",
    "    rows_df = rows_df[rows_df['group_index'].apply(lambda x: x in rows_w_numbers)]\n",
    "    \n",
    "    prev_x_coord = rows_df.groupby('group_index')['coord_item'].shift().apply(lambda x: x[0] + x[2] if x == x else None)\n",
    "    distance_from_prev_x_coord = (rows_df['coord_item'].apply(lambda x: x[0]) - prev_x_coord).fillna(0)\n",
    "    rows_df['distance_from_leftmost_val'] = distance_from_prev_x_coord\n",
    "\n",
    "    '''\n",
    "    from scipy.stats import zscore\n",
    "    \n",
    "    std_group = rows_df.groupby('group_index')['distance_from_leftmost_val'].std() > 250\n",
    "    groups_w_high_std = set(std_group[std_group == True].index) # b/c of high variance, most likely has row values \n",
    "    test_df = rows_df[rows_df['group_index'].apply(lambda x: x in groups_w_high_std)]\n",
    "    \n",
    "    test_df['z_scores'] = test_df.groupby(['group_index'])['distance_from_leftmost_val'].transform(lambda x : abs(zscore(x, ddof=1)))\n",
    "    test_df['header_bool'] = test_df['z_scores'] < 1 \n",
    "    \n",
    "    test_df.groupby(['group_index', 'header_bool']).agg({\n",
    "        'text': lambda x: x if len(x) == 1 else ' '.join(x),\n",
    "        'group_index': set(),\n",
    "        'coord_item': lambda x: x\n",
    "    }).reset_index().sort_values(['group_index', 'header_bool'], ascending = [True, False])\n",
    "    '''\n",
    "    \n",
    "    pulled_values_df = rows_df.copy()[['text', 'group_index','coord_item', 'distance_from_leftmost_val']]\n",
    "    pulled_values_df['breakpoints'] = (pulled_values_df['distance_from_leftmost_val'] > y_margin) | (pulled_values_df['distance_from_leftmost_val'] == 0)\n",
    "    \n",
    "    group_list = [] \n",
    "    i = -1 \n",
    "    for val in list(pulled_values_df['breakpoints']):\n",
    "        if val == True: \n",
    "            i += 1 \n",
    "        group_list.append(i)\n",
    "    \n",
    "    pulled_values_df['temp_grouper'] = group_list\n",
    "    \n",
    "    pulled_values_df = pulled_values_df.groupby(['group_index', 'temp_grouper']).agg({\n",
    "        'text': lambda x: ' '.join(x),\n",
    "        'group_index': set(),\n",
    "        'coord_item': lambda x: list(x)\n",
    "    }).reset_index().drop('temp_grouper', axis = 1)\n",
    "    \n",
    "    pulled_values_df['header_cond'] = ~pulled_values_df['group_index'].duplicated()\n",
    "    header_df = pulled_values_df[pulled_values_df['header_cond'] == True]\n",
    "    data_df = pulled_values_df[pulled_values_df['header_cond'] == False]\n",
    "    \n",
    "    header_df['grouped_header_coord'] = header_df['coord_item'].apply(lambda x: group_up_coord(list(x)))\n",
    "    header_df = header_df.rename({'text': 'header', 'grouped_header_coord': 'header_coord'}, axis = 1)\n",
    "    header_df = header_df[['group_index', 'header', 'header_coord']]\n",
    "    data_df['coord_item'] = data_df['coord_item'].apply(lambda x: group_up_coord(list(x)))\n",
    "    data_df = data_df.drop('header_cond', axis = 1)\n",
    "    \n",
    "    data_df = pd.merge(\n",
    "        left = header_df,\n",
    "        right = data_df,\n",
    "        on = 'group_index', \n",
    "        how = 'left'\n",
    "    )\n",
    "    \n",
    "    extracted_row_df = pd.merge(\n",
    "        left = categorized_df[['group_index', 'predicted_category', 'prediction_confidence', 'file']],\n",
    "        right = data_df,\n",
    "        on = 'group_index',\n",
    "        how = 'right'\n",
    "    )\n",
    "\n",
    "    def valid_data(x):\n",
    "        if x == x:\n",
    "            return any(char.isdigit() for char in x) | (x == '-')\n",
    "        else: # null value found, still return \n",
    "            return True\n",
    "\n",
    "    extracted_row_df = extracted_row_df[extracted_row_df['text'].apply(lambda x: valid_data(x))] # limits to rows with at least 1 number\n",
    "    \n",
    "    extracted_row_df = extracted_row_df.rename({'text': 'extracted_value' , 'coord_item': 'final_coord_block_par_line_group'}, axis = 1)\n",
    "    extracted_row_df = extracted_row_df.sort_values(['file','predicted_category', 'prediction_confidence'], ascending=[True, True, False])\n",
    "    \n",
    "    return extracted_row_df\n",
    "\n",
    "def categorize_nav(df):\n",
    "    # RECATEGORIZING NAVs AFTER NUMBERS HAVE BEEN PULLED; ASSUME FIRST TWO ROWS WITH NUMERIC VALUES ARE THE BEG_NAV/END_NAV\n",
    "    # TODO: what happens if beg_nav is found and not end_nav or vice versa?\n",
    "    def nav_categorizer(x, group_set): \n",
    "        if x['predicted_category'] != 'nav':\n",
    "            return True\n",
    "        else: \n",
    "            return x['group_index'] in group_set\n",
    "    \n",
    "    extracted_row_df = df.copy()\n",
    "\n",
    "    # finds first two predicted rows (highest confidence)\n",
    "    top_nav_groups_w_num = set(extracted_row_df[extracted_row_df['predicted_category'] == 'nav']['group_index'].drop_duplicates()[:2]) \n",
    "    #top_nav_groups_w_num = set(extracted_row_df[extracted_row_df['predicted_category'] == 'nav'].dropna(subset = 'extracted_value')['group_index'].drop_duplicates()[:2]) # YOU BETTER HOPE YOU CAUGHT A BEG_NAV/END_NAV\n",
    "    # ^ this commented out portion doesnt work, what if a value wasnt found? you're going to have to write something that only accounts for '-' if its the last things that are present (not sandwiched)\n",
    "    extracted_row_df = extracted_row_df[extracted_row_df.apply(lambda x: nav_categorizer(x, top_nav_groups_w_num), axis =1)] # removing all navs except first two (top 2 confidence)\n",
    "\n",
    "    # categorizes these 2 rows into beg_nav and end_nav based on x_coord\n",
    "    nav_rows = extracted_row_df[extracted_row_df['predicted_category'] == 'nav']\n",
    "    nav_rows['x_coord'] = nav_rows['final_coord_block_par_line_group'].apply(lambda x: x[1] if x == x else x)\n",
    "    nav_rows = nav_rows.sort_values(['x_coord'])\n",
    "    beg_nav_group_num = nav_rows[['group_index']].drop_duplicates()['group_index'].iloc[0]\n",
    "    nav_rows['predicted_category'] = nav_rows['group_index'].apply(lambda x: 'beg_nav' if x == beg_nav_group_num else 'end_nav')\n",
    "    extracted_row_df[extracted_row_df['predicted_category'] == 'nav'] = nav_rows\n",
    "\n",
    "    return extracted_row_df\n",
    "    \n",
    "def extract_data(file, pdf_img_dict, classifier):\n",
    "    '''\n",
    "    a. pull text data using tesseract w/ function generate_text_data\n",
    "    b. predict row categories using get_predictions()\n",
    "    c. select rows based on predetermined rules defined in category_selection()\n",
    "    d. narrow down rows even more by minimum threshold: currently defined as 40% \n",
    "    e. pull numerical data from each row \n",
    "    f. make selection from categorized nav rows into singular beg_nav and end_nav; this is done later b/c we only want to make selection from rows w/ numerical data\n",
    "    g. generate a num_cols column which is basically the number of data points in each row. this is used to only select QTD data (only one column)\n",
    "    h. predict whether QTD data is on the left or right side, narrow down data based on prediction. NOTE: this assumes QTD is only 1 column, if its 2+ columns you're SOL (itll always pick the first or last of each row)\n",
    "    '''\n",
    "    \n",
    "    # 3a\n",
    "    cleaned_data_df = generate_text_data(pdf_img_dict[file])\n",
    "\n",
    "    #3b\n",
    "    high_confidence_img, best_img, high_confidence_df, best_guess_df, raw_predictions_df = get_predictions(pdf_img_dict[file], cleaned_data_df, classifier)\n",
    "    # ^ note that high_confidence_img, best_img, high_confidence_df, and best_guess_df are not currently necessary. potentially remove in the future \n",
    "\n",
    "    #3c\n",
    "    categorized_df = raw_predictions_df.groupby('predicted_category').apply(lambda x: category_selection(x)).reset_index(drop = True).sort_values(['predicted_category', 'prediction_confidence'], ascending = [True, False])\n",
    "    categorized_df['file'] = [file for x in range(len(categorized_df))]\n",
    "\n",
    "    #3d\n",
    "    prev_len = len(categorized_df)\n",
    "    categorized_df = categorized_df[categorized_df['prediction_confidence'] > .4]\n",
    "    if len(categorized_df) != prev_len:\n",
    "        print(' - dropped some predicted values due to low confidence')\n",
    "    categorized_df = categorized_df.sort_values(['predicted_category', 'prediction_confidence'], ascending = [True, False])\n",
    "\n",
    "    #3e\n",
    "    extracted_row_df = extract_row_data(cleaned_data_df, categorized_df)\n",
    "    \n",
    "    #3f\n",
    "    extracted_row_df = categorize_nav(extracted_row_df)\n",
    "\n",
    "    #3g\n",
    "    extracted_row_df = extracted_row_df.drop(['final_coord_block_par_line_group'], axis = 1)\n",
    "    extracted_row_df = extracted_row_df.sort_values(['file','predicted_category', 'prediction_confidence'], ascending=[True, True, False])\n",
    "    extracted_row_df = pd.merge(\n",
    "        left = extracted_row_df, \n",
    "        right = extracted_row_df.groupby('group_index').size().rename('num_cols'), \n",
    "        on = 'group_index'\n",
    "    )\n",
    "\n",
    "    #3h \n",
    "    # guessing quarter location (left or right)\n",
    "    location_guess = generalize_quarter_area(cleaned_data_df)\n",
    "    print(' - data we want is on the \\'' + location_guess + '\\' side')\n",
    "    if 'left' in location_guess:\n",
    "        narrowed_df = extracted_row_df.drop_duplicates('group_index', keep = 'first')\n",
    "    else:\n",
    "        narrowed_df = extracted_row_df.drop_duplicates('group_index', keep = 'last')\n",
    "\n",
    "    extracted_row_df['guessed_quarter_loc'] = [location_guess for _ in range(len(extracted_row_df))]\n",
    "\n",
    "    return cleaned_data_df, raw_predictions_df, categorized_df, extracted_row_df, narrowed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5eafd54b-ed18-4683-9b52-50e00c3da4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "logistic classifier testing accuracy 0.9706959706959707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "omni_conxn_str = 'mysql+pymysql://bwong:?(}9LcsW@analytics-proxy.stepstoneapps.com/spar_analytics'\n",
    "lgclf = gen_model(omni_conxn_str)\n",
    "print()\n",
    "\n",
    "file_list = None\n",
    "if True: # example of sub-sample of files\n",
    "    ishaan_df = pd.read_excel('cab_validation_ishaan.xlsx')\n",
    "    ishaan_df = ishaan_df[ishaan_df['Assigned'] == 'Ishaan']\n",
    "    file_list = list(set(ishaan_df['file']))\n",
    "    file_list.sort()\n",
    "input_folder = 'text_extracting_folder_test'\n",
    "pdf_img_dict = generate_pdf_img_dict(input_folder, file_list = file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b444df66-06fd-4e86-9d0b-5eb1070f71c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************2021.09.30.CAB.FTV III.SS Intl IV Guernsey.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{24, 16}\n",
      " - data we want is on the 'left*' side\n",
      "************2021.09.30.CAB.FTV Vagaro.HESTA.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'contributions', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{14, 7}\n",
      " - data we want is on the 'left*' side\n",
      "************2021.09.30.CAB.Fortress Japan II Yen B.Mesirow Intl.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      " - dropped some predicted values due to low confidence\n",
      "testing new extraction method!!!\n",
      "{5, 31}\n",
      " - data we want is on the 'left' side\n",
      "************2021.09.30.CAB.Fortress Japan III Dollar B.CIRS.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      " - dropped some predicted values due to low confidence\n",
      "testing new extraction method!!!\n",
      "{5, 30}\n",
      " - data we want is on the 'left' side\n",
      "************2021.09.30.CAB.HarbourVest IPEP III Partnership.PASERS.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{16, 25}\n",
      " - data we want is on the 'left' side\n",
      "************2021.09.30.CAB.HarbourVest NYSTRS CoInv III.NYSTRS.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{25, 18}\n",
      " - data we want is on the 'left' side\n",
      "************2021.09.30.CAB.Insight II.LAFPP.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{19, 7}\n",
      " - data we want is on the 'left*' side\n",
      "************2021.09.30.CAB.Insight XII Buyout Annex.NYC FIRE.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      " - dropped some predicted values due to low confidence\n",
      "testing new extraction method!!!\n",
      "{34, 3}\n",
      " - data we want is on the 'left*' side\n",
      "************2021.09.30.CAB.Lone Star IX.Bermuda.HKJC Trust.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'unfunded', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{9, 11}\n",
      " - data we want is on the 'left*' side\n",
      "************2021.09.30.CAB.Lone Star RE III.Bermuda.GOSI.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      " - dropped some predicted values due to low confidence\n",
      "testing new extraction method!!!\n",
      "{6, 7}\n",
      " - data we want is on the 'left*' side\n",
      "************2021.09.30.CAB.Madison Inda IV.SSOF II Off.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{9, 12}\n",
      " - data we want is on the 'left*' side\n",
      "************2021.09.30.CAB.Madison Realty Debt IV.Texas Schools.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{9, 21}\n",
      " - data we want is on the 'left*' side\n",
      "************2021.09.30.CAB.Maple H2.NYSTRS.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{25, 19}\n",
      " - data we want is on the 'left' side\n",
      "************2021.09.30.CAB.Maple H3.NYSTRS.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{16, 26}\n",
      " - data we want is on the 'left' side\n",
      "************2021.09.30.CAB.New Mountain II.NYCFIRE.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{19, 7}\n",
      " - data we want is on the 'left*' side\n",
      "************2021.09.30.CAB.Providence Sedona CoInv.EV41.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{16, 13}\n",
      " - data we want is on the 'left' side\n",
      "************2021.09.30.CAB.Providence Strategic Growth V.UN.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{17, 12}\n",
      " - data we want is on the 'left' side\n",
      "************2021.09.30.CAB.Rockpoint Growth Income Side Car II-F.NCRS.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'nav'}\n",
      " - dropped some predicted values due to low confidence\n",
      "testing new extraction method!!!\n",
      "{1, 5}\n",
      " - data we want is on the 'left*' side\n",
      "************2021.09.30.CAB.Rockpoint Growth Income Side Car.NCRS.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{1, 5}\n",
      " - data we want is on the 'left*' side\n",
      "************2021.09.30.CAB.SRE Care.SREP III.pdf************\n",
      "pre-cleaning data...\n",
      " - dropping null values\n",
      " - lowercasing, removing punctuation, removing numbers\n",
      " - remove empty strings\n",
      "getting category predictions...\n",
      " - categories found: {'distributions', 'unfunded', 'contributions', 'nav'}\n",
      "testing new extraction method!!!\n",
      "{8, 2}\n",
      " - data we want is on the 'left' side\n"
     ]
    }
   ],
   "source": [
    "narrowed_final_df = pd.DataFrame()\n",
    "expanded_final_df = pd.DataFrame()\n",
    "categorized_df_dict = {} # used for debugging classifier = lgclf\n",
    "for file in file_list: #, '2021.09.30.CAB.HarbourVest IPEP III Partnership.PASERS.pdf']: # '2021.09.30.CAB.Fortress Japan II Yen B.Mesirow Intl.pdf' '2021.09.30.CAB.FTV III.SS Intl IV Guernsey.pdf'\n",
    "    print('************' + file + '************')\n",
    "    cleaned_data_df, raw_predictions_df, categorized_df, extracted_row_df, narrowed_df = extract_data(file, pdf_img_dict, lgclf)\n",
    "\n",
    "    categorized_df_dict[file] = categorized_df # used for debugging; has raw data before doing column-data extraction \n",
    "    expanded_final_df = pd.concat([expanded_final_df, extracted_row_df])\n",
    "    narrowed_final_df = pd.concat([narrowed_final_df, narrowed_df])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0e408093-5b93-4b20-b269-79e513ca41bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021.09.30.CAB.FTV Vagaro.HESTA.pdf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_index</th>\n",
       "      <th>predicted_category</th>\n",
       "      <th>prediction_confidence</th>\n",
       "      <th>file</th>\n",
       "      <th>header</th>\n",
       "      <th>header_coord</th>\n",
       "      <th>extracted_value</th>\n",
       "      <th>num_cols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>beg_nav</td>\n",
       "      <td>0.895392</td>\n",
       "      <td>2021.09.30.CAB.FTV Vagaro.HESTA.pdf</td>\n",
       "      <td>Balance September 30, 2021</td>\n",
       "      <td>[803, 3327, 973, 72]</td>\n",
       "      <td>3,333,333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>contributions</td>\n",
       "      <td>0.984795</td>\n",
       "      <td>2021.09.30.CAB.FTV Vagaro.HESTA.pdf</td>\n",
       "      <td>Capital Contributions</td>\n",
       "      <td>[895, 1899, 728, 72]</td>\n",
       "      <td>3,333,333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>end_nav</td>\n",
       "      <td>0.845964</td>\n",
       "      <td>2021.09.30.CAB.FTV Vagaro.HESTA.pdf</td>\n",
       "      <td>Balance July 1, 2021</td>\n",
       "      <td>[803, 1594, 682, 72]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   group_index predicted_category  prediction_confidence   \n",
       "0           14            beg_nav               0.895392  \\\n",
       "1            8      contributions               0.984795   \n",
       "2            7            end_nav               0.845964   \n",
       "\n",
       "                                  file                      header   \n",
       "0  2021.09.30.CAB.FTV Vagaro.HESTA.pdf  Balance September 30, 2021  \\\n",
       "1  2021.09.30.CAB.FTV Vagaro.HESTA.pdf       Capital Contributions   \n",
       "2  2021.09.30.CAB.FTV Vagaro.HESTA.pdf        Balance July 1, 2021   \n",
       "\n",
       "           header_coord extracted_value  num_cols  \n",
       "0  [803, 3327, 973, 72]       3,333,333         1  \n",
       "1  [895, 1899, 728, 72]       3,333,333         1  \n",
       "2  [803, 1594, 682, 72]             NaN         1  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_file = file_list[1]\n",
    "print(curr_file)\n",
    "\n",
    "narrowed_final_df[narrowed_final_df['file'] == curr_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a03aac17-6173-4eb3-b789-70272565ee32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Schedule of Partner's Capital Account - (Unaudited)\""
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "narrowed_final_df[narrowed_final_df['file'] == curr_file]['header'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94deb10e-1be0-4217-b3fe-c3e4fc73e953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
